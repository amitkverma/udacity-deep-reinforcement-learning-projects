{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "# Seed\n",
    "SEED=1211\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# device info\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of state: 37\n",
      "Size of action: 4\n",
      "Number of agents: 1\n",
      "States look like: [ 1.          0.          0.          0.          0.84408134  0.          0.\n",
      "  1.          0.          0.0748472   0.          1.          0.          0.\n",
      "  0.25755     1.          0.          0.          0.          0.74177343\n",
      "  0.          1.          0.          0.          0.25854847  0.          0.\n",
      "  1.          0.          0.09355672  0.          1.          0.          0.\n",
      "  0.31969345  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# environment details\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "print('Size of state:', state_size)\n",
    "print('Size of action:', action_size)\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "# examine the state space \n",
    "print('States look like:', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Introduction\n",
    "Prioritized DQN is one of the improvements over Double DQNs.It was first discussed on paper [Prioritized Experience Replay (PER)](https://arxiv.org/abs/1511.05952) which was introduced in 2015 by Tom Schaul. The basic idea is that some experiences may be more important than others for our training, but it might occur less frequently. Because we sample the batch randomly these rich experiences will rarely get selected or no chance to be selected. Hence, we add priority with each experience and sample based on priority.\n",
    "\n",
    "#### 3.1 Revisting the Double DQN algorithm\n",
    "\n",
    "DQN algorithm attempts to find a policy $\\pi$ which maps a given state $s_t$ to an action at such that it maximizes the expected reward of the agent $\\mathbb{E}_{\\pi}\\Big[ \\sum_{t=0}^\\infty r_t \\Big]$ from some starting state $s_0$. DQN obtains $\\pi$ implicitly by calculating a state-value function $Q_Î¸(s,a)$ parameterized by Î¸, which measures the goodness of the given state-action with respect to some behavioral policy.\n",
    "\n",
    "To find an appropriate Î¸, which then determines the final policy $\\pi$, DQN performs the following optimization:\n",
    "\n",
    "<center>\n",
    "$\n",
    "{\\rm minimize}_{\\theta} \\;\\; \\mathbb{E}_{(s_t,a_t,r_t,s_{t+1})\\sim D}\n",
    "\\left[\n",
    "\\Big(r_t + \\gamma \\max_{a \\in \\mathcal{A}} Q_{\\theta^-}(s_{t+1},a) - Q_\\theta(s_t,a_t)\\Big)^2\n",
    "\\right]\n",
    "$\n",
    "</center>\n",
    "\n",
    "\n",
    "where $(s_t,a_t,r_t,s_{t+1})$ are batches of samples from the replay buffer $D$, which is designed to store the past N samples. In addition, $A$ represents the set of discrete actions, Î¸ is the local network and $\\theta^-$ represents the target network. Both networks use the same architecture, and we use $Q_Î¸(s,a)$ or $Q_{\\theta^-}(s,a)\n",
    "$ to denote which of the two is being applied to evaluate (s,a).\n",
    "\n",
    "The target network starts off by getting matched to the current network, but remains frozen (usually for thousands of steps) before getting updated again to match the network. The process repeats throughout training, with the goal of increasing the stability of the targets $r_t + \\gamma \\max_{a \\in \\mathcal{A}}Q_{\\theta^-}(s_{t+1},a)$.\n",
    "\n",
    "#### 3.2 Temporal difference (or TD) error\n",
    "The difference between the estimated reward at any given state and the actual reward received. The larger the error function, the larger the difference between the expected and actual reward.\n",
    "TD Error for vanilla DQN:\n",
    "\n",
    "<center>\n",
    "$\\delta_i = r_t + \\gamma \\max_{a \\in \\mathcal{A}} Q_{\\theta^-}(s_{t+1},a) -\n",
    "Q_\\theta(s_t,a_t)$\n",
    "</center>\n",
    "\n",
    "and for Double-DQN:\n",
    "\n",
    "<center>\n",
    "$\\delta_i = r_t + \\gamma Q_{\\theta^-}(s_{t+1},{\\rm argmax}_{a \\in \\mathcal{A}}\n",
    "Q_\\theta(s_{t+1},a)) - Q_\\theta(s_t,a_t)$\n",
    "</center>\n",
    "\n",
    "Given the magnitude of the TD error $| \\delta_i |$. We can calculate the priority $p_i$ by adding $\\epsilon$ to TD error magnitude.\n",
    "\n",
    "\n",
    "<center>\n",
    "$p_i = | \\delta_i | + \\epsilon$    \n",
    "</center>\n",
    "\n",
    "where $\\epsilon$ is a small constant ensuring that the sample has some non-zero probability of being drawn.\n",
    "\n",
    "But we canâ€™t do greedy prioritization, because it will lead to always training the same experiences (that have big priority), and then we'll be over-fitting our agent. So, we will use stochastic prioritization, which generates the probability of being chosen for a replay:\n",
    "\n",
    "<center>\n",
    "$P(i) = \\frac{p_i^a}{\\sum_k p_k^a}$\n",
    "</center>\n",
    "\n",
    "Where:\n",
    "- $p_i$ - Priority value\n",
    "- $\\sum_k p_k^a$ - Normalization by all priority values in Replay Buffer\n",
    "- $ð‘Ž$ - Hyperparameter used to reintroduce some randomness in the experience selection for the replay buffer (if a=0 pure randomness, if a=1 only selects the experience with the highest priorities)\n",
    "\n",
    "Now, we can put this priority $P(i)$ in our replay buffer tuple $(s_{t},a_{t},r_{t},s_{t+1}, P(i))$. \n",
    "\n",
    "### Model\n",
    "Fully connected NN layer with 1 hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    \"\"\" Deep Q-Network \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, fc1_size=64, fc2_size=64):\n",
    "        \"\"\"Initialize parameters and fc model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size : Dimension of each state\n",
    "            action_size : Dimension of each action\n",
    "            seed : Random seed\n",
    "            fc1_size : Number of nodes in first hidden layer\n",
    "            fc2_size : Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sum Tree\n",
    "\n",
    "According to the paper, when we are using Proportional Prioriziation for $P(i)$ then we should use \"sum-tree\" data structure for storing a pair of transition and $P(i)$. SumTree helps us select some numbers randomly according to their probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "from collections import deque\n",
    "\n",
    "class SumTree:\n",
    "    \"\"\" Story data with its priority in the tree \"\"\"\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\" We initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "        Params\n",
    "        ======\n",
    "            capacity : for all priority values\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prioritized Buffer\n",
    "\n",
    "Since samples that have high priority are more likely to be selected for many time in our training in comparison to low priority experience. So, as a consequence, we will update our weights with only a small portion of experience that we consider to be interesting. To correct this we will use importance sampling weight where we adjust (or reduce). the weight of ofen seen samples.\n",
    "\n",
    "\n",
    "<center>\n",
    "$(\\frac{1}{N} \\times \\frac{1}{P(i)})^b$\n",
    "</center>\n",
    "\n",
    "Where:\n",
    "- ð‘ - Replay Buffer Size\n",
    "- ð‘ƒ(ð‘–) - Sampling probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedBuffer:\n",
    "    \"\"\" Samples the batch data according to priority \"\"\"\n",
    "\n",
    "    def __init__(self, max_size, alpha=0.6, beta=0.4):\n",
    "        \"\"\" Initialize the sum_tree and hyperparameters alpha and beta\n",
    "        Params\n",
    "        ======\n",
    "            max_size : for all priority values\n",
    "        \"\"\"\n",
    "        self.sum_tree = SumTree(max_size)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.current_length = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        priority = 1.0 if self.current_length is 0 else self.sum_tree.tree.max()\n",
    "        self.current_length = self.current_length + 1\n",
    "        experience = (state, action, np.array([reward]), next_state, done)\n",
    "        self.sum_tree.add(priority, experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_idx, batch, IS_weights = [], [], []\n",
    "        segment = self.sum_tree.total() / batch_size\n",
    "        p_sum = self.sum_tree.tree[0]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.sum_tree.get(s)\n",
    "\n",
    "            batch_idx.append(idx)\n",
    "            batch.append(data)\n",
    "            prob = p / p_sum\n",
    "            IS_weight = (self.sum_tree.total() * prob) ** (-self.beta)\n",
    "            IS_weights.append(IS_weight)\n",
    "\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "\n",
    "        for transition in batch:\n",
    "            state, action, reward, next_state, done = transition\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "\n",
    "        return (state_batch, action_batch, reward_batch, next_state_batch, done_batch), batch_idx, IS_weights\n",
    "\n",
    "    def update_priority(self, idx, td_error):\n",
    "        priority = td_error ** self.alpha\n",
    "        self.sum_tree.update(idx, priority)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.current_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedDQN:\n",
    "    \"\"\" Prioritized DQN Agent \"\"\"\n",
    "\n",
    "    def __init__(self,state_size, action_size , learning_rate=3e-4, gamma=0.99, buffer_size=10000):\n",
    "        \"\"\" Initialize the sum_tree and hyperparameters alpha and beta\n",
    "        Params\n",
    "        ======\n",
    "            state_size : state size\n",
    "            action_size : action size\n",
    "            learning_rate: learning rate\n",
    "            gamma:  discount factor\n",
    "            buffer_size: replay buffer size\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        self.replay_buffer = PrioritizedBuffer(buffer_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")        \n",
    "        self.model = DQNetwork(state_size, action_size).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.MSE_loss = nn.MSELoss()\n",
    "\n",
    "    def act(self, state, eps=0.0):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model(state)\n",
    "        self.model.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def _sample(self, batch_size):\n",
    "        \"\"\" Get Samples from replay buffer\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            batch_size : batch size\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.replay_buffer.sample(batch_size)\n",
    "\n",
    "    def _compute_TDerror(self, batch_size):\n",
    "        transitions, idxs, IS_weights = self._sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = transitions\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        IS_weights = torch.FloatTensor(IS_weights).to(self.device)\n",
    "\n",
    "        curr_Q = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        curr_Q = curr_Q.squeeze(1)\n",
    "        next_Q = self.model(next_states)\n",
    "        max_next_Q = torch.max(next_Q, 1)[0]\n",
    "        expected_Q = rewards.squeeze(1) + self.gamma * max_next_Q\n",
    "\n",
    "        td_errors = torch.pow(curr_Q - expected_Q, 2) * IS_weights\n",
    "\n",
    "        return td_errors, idxs\n",
    "\n",
    "    def update(self, batch_size):\n",
    "        td_errors, idxs = self._compute_TDerror(batch_size)\n",
    "\n",
    "        # update model\n",
    "        td_errors_mean = td_errors.mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        td_errors_mean.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update priorities\n",
    "        for idx, td_error in zip(idxs, td_errors.cpu().detach().numpy()):\n",
    "            self.replay_buffer.update_priority(idx, td_error)\n",
    "            \n",
    "    \n",
    "    def print(self):\n",
    "        num_parameters = sum([params.numel() for params in self.model.parameters() if params.requires_grad]);\n",
    "        print(self.model)\n",
    "        print(f\"Number of parameters in model : {num_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNetwork(\n",
      "  (fc1): Linear(in_features=37, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "Number of parameters in model : 6852\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "GAMMA = 0.99            # discount factor\n",
    "LR = 5e-3               # learning rate \n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "max_t = 500\n",
    "eps_start=1.0\n",
    "eps_decay=.995\n",
    "eps_min=0.05\n",
    "n_episodes = 1000\n",
    "\n",
    "agent = PrioritizedDQN(state_size, action_size)\n",
    "agent.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, n_episodes, max_t, eps_start, eps_min, eps_decay):\n",
    "\n",
    "    scores = []                                               # list containing scores from each episode                 \n",
    "    scores_window = deque(maxlen=100)                         # store only the last 100 scores\n",
    "    eps = eps_start                                           # initialize epsilon (for epsilon-greedy policy)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment\n",
    "        state = env_info.vector_observations[0]               # get the initial state\n",
    "        score = 0                                             # initialize the score\n",
    "\n",
    "        for t in range(max_t):                                # run for maximum of max_t timesteps \n",
    "            action = agent.act(state, eps)                    # select the action\n",
    "            env_info = env.step(action)[brain_name] \n",
    "            next_state = env_info.vector_observations[0]      # get the state\n",
    "            reward = env_info.rewards[0]                      # get the reward\n",
    "            done = env_info.local_done[0]                     # whether the episode is complete or not\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            score += reward\n",
    "\n",
    "            if len(agent.replay_buffer) > BATCH_SIZE:\n",
    "                agent.update(BATCH_SIZE)   \n",
    "\n",
    "            if done or t == max_t-1:\n",
    "                scores.append(score)\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        scores_window.append(score)                # update the window of scores\n",
    "        scores.append(score)                       # update the list of scores\n",
    "        eps = max(eps_min, eps * eps_decay)        # modify epsilon\n",
    "        average_score = np.mean(scores_window)\n",
    "        print('\\rEpisode {} \\tAverage score: {: .2f}'.format(i_episode, average_score), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {} \\tAverage score: {: .2f}'.format(i_episode, average_score))            \n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 \tAverage score:  0.07\n",
      "Episode 200 \tAverage score:  1.07\n",
      "Episode 300 \tAverage score:  1.37\n",
      "Episode 400 \tAverage score:  2.00\n",
      "Episode 500 \tAverage score:  2.21\n",
      "Episode 600 \tAverage score:  3.80\n",
      "Episode 700 \tAverage score:  1.52\n",
      "Episode 800 \tAverage score:  3.07\n",
      "Episode 900 \tAverage score:  2.11\n",
      "Episode 1000 \tAverage score:  2.14\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztnXd8HcXV93/nSpZkuclFNu422FQbsK3Qu4FQEkiHkBBSeZKQUFKIgeRJecibhAQISUgoiYEQsAmdYCAY4wruvfeCu9wly5Isad4/dvfeLbO7s3t3797re76fjy1pd3bm7OzsOTNnZs6SEAIMwzAMk0paAIZhGCY/YIPAMAzDAGCDwDAMw+iwQWAYhmEAsEFgGIZhdNggMAzDMADYIDAMwzA6bBAYhmEYAGwQGIZhGJ3SpAUIQo8ePcSgQYOSFoNhGKagmD9//h4hRLVfuoIyCIMGDcK8efOSFoNhGKagIKLNKunYZcQwDMMAYIPAMAzD6LBBYBiGYQCwQWAYhmF02CAwDMMwANggMAzDMDpsEBiGYRgAbBAYhmEsNDS3YOyMjTjUeDRpUXIOGwSGYRgTry3cjl+9uQLPzdqStCg5hw0CwzCMCWNkcKChOWFJcg8bBIZhGAYAGwSGYRhGhw0CwzAMA4ANAsMwDKPDBoFhGIYBwAaBYRiG0WGDwDAMwwBgg8AwDMPosEFgGIZhAOTAIBDRWCLaTUTLTMe6EdFEIlqr/+watxwMwzCMN7kYITwN4CrbsTEAJgkhhgKYpP/NMAzDJEjsBkEIMQ3APtvh6wE8o//+DIBPxS0HwxQiBxuO4r0VuyCESFoUpghIag6hlxBiBwDoP3u6JSSiW4loHhHNq62tzZmADJMP3D9hBb75z3nYsOdw0qIwRUDeTyoLIZ4QQtQIIWqqq6uTFodhcsrSbQcBAE1H2xKWhCkGkjIIu4ioNwDoP3cnJAfD5DVEBAAQYJcREz9JGYQ3ANyi/34LgNcTkoNhCgKeQmByQS6WnY4DMBPASUS0lYi+AeC3AK4gorUArtD/ZhjGBiUtAFNUlMZdgBDiiy6nRsddNsMUOsQWgckheT+pzDDFjGEQ2GXE5AI2CAyTx5DuNGpji8DkADYIDFMAsDlgcgEbBIbJYzIuIzYJTPywQWCYPMaYU25je8DkADYIDJPPpJcZsUVg4ocNAsPkMTxCYHIJGwSGyYIdB49g16FGAMDqnXU40tzqSHO0tQ3Lth0MNQ9gX3a682Ajdh5sdKRbt7sO9U0tgfPPBWt31eGwh2zmOmSShQ0Cw2TBlQ9Pw/V/+QAHGprx8T9Ow09fW+ZI848ZG/GJP8/AnI32KPDqGMbk6kem4RN/nmE513i0FZc/NA13vbAodP5x0dDcgisenoYfv7TYNc3HH56G6/4yw/U8kzti36nMMMcydY0tqGtswZGj2sjg3RU7AZxhSbNk6wEAQG19U+D87S6j/Q1HHWla9JMTV+wKnH/cNLdoUVrfWrrTNc2hxhYcaszP0U2xwSMEhskRFCIykUq0U16SykQFGwSGiYC4dHLahBSozmdbVViwQWCYmMlGKRqTyl6rjFjnMlHBBoFhckSYyKWGm8nbZRRWovjJY9EYCWwQGCYChOMX07lstKLCCIFhooINAsPEjNG7z+bTBp4Tx2wsmIhgg8AwUeKh9cO5jDQKVefzCqjCgg0Cw0RJxC4jlWinXvMLDBMENggMEwFqPeEQ+xCMSeUC1fkFKnbRkqhBIKK7iGg5ES0jonFEVJGkPAyTNRF/A1lp2SlrXSYiEjMIRNQXwO0AaoQQwwCUALgxKXkYJhJkLqMosmWtz+SApF1GpQDaE1EpgEoA2xOWh2FCoaKvQ00qm0YI+w43p48fbDia/jufTUUQO7bj4BG0tLZhv+k+DYx7bW0TONDgPJ/v1De1oKklEwn3UOPRdJynfCKx4HZCiG1E9AcAWwAcAfCuEOLdpORhmLjIpnNfVqL12e6fsAJb9x9JHz/jV9qr8sp3z8PAbpVZyZcvnPub99O/z7lvNHp20jzIk1buwjeemYcnbh6F6Wv34NlZm7HyV1ehfVlJUqIG5tI/TMHxPTrghf85FwBw/m/ex6hBXfH0185KWDIrSbqMugK4HsBgAH0AdCCiL0vS3UpE84hoXm1tba7FZJjICDO90F9X9mZjYGbz3sNZSBQ/YVdAHTqSiX66dNtBAMCybQfx7KzNALSQ34VEbV0TZpvCn9c1tWDK6vzTZ0m6jC4HsFEIUSuEOArgFQDn2RMJIZ4QQtQIIWqqq6tzLiTDJImfESFQXruMoiA9wgrjc2MCkaRB2ALgHCKqJC3G72gAKxOUh2FiIrzKJh8leOzqSGedkedZJgoSMwhCiNkAXgKwAMBSXZYnkpKHYeImLiWW1wuQIpAtn2/vWCPRL6YJIX4O4OdJysAwcZPXCjtP4TpLhqSXnTLMMYFK7Lm4lFw+h66Ib1SUv/dcyLBBYJicEb0S85tjOCbQlX8x3GrSsEFgmJgxerOxdWqPwc6y7JbCfJOaCQYbBIaJABW3TRx6O99VZBRGUJZFIdnAQnJvsUFgmBwRh14gKizlqIqsrgrVZVRIX7tjg8AwMZOeVI5JdedzBzSKe87n+1OhtYAsAhsEhokAFaUVRrH5uRuKwa8u+wRpIRmJtgISlg0CU3C0tolY/LJtAXtyZhm8XnrjlJFGtZyW1jbf/rXmMoqmLoLef1CE0J6bSjnmezKSF45ateI1QlCt87ifjQEbBKag+Pfcj3DCvW/hs3/7MNJ8n5y2AcN+8V80NLf4J9b5/riF6d8ve3Cqa7qZG/YCAO4Yvwi//M9yXPC7913TGjz07moMue9tLNiyX1mebHh44hqM+L+JONoabUhms5385jPz8JOXl+CqR6YpX/+PGRvxtynrAQAPTVwTqWy5YtehRtdzp/zvO1hfW+95/S/eWI6Lfj85arGkJLpTmWGC8szMTQCABVsORJrvH99bg4bmVtQ1tqCyTO21eHPJDqV0PTuVp6OVPvXBJqVrpq7RImFuc4lyakCIxn3yyKS1AICmlja0K4mnnzhp1W7ltMY9PTp5nfx8AY0XDh456nquqaUN63bX44Tqjq5pnv5wUwxSyeERAlNQxLXSJM7dxGHyVHWTRF0fUbviCkdtx0ch1QEbBIaJmTCTiobfWWmyOnDuxwAFdNO8D4FhihTZqx/GILSldzf7jhEC5+1FvqiuAtKhvvjdSz6tE2ODwDAm4vBNyxaI+Cl6oegyUskrSfJZtlxRSDXABoFhIkTW25MpRT892aZoEYii7U3ni/72M8x5IqYSuVoyGgVsEBjGRLYKUe4yUktnvUYopYuciAvMFwOTJP4LA/LHacQGgWFiRjaHoOwy8t2pzOQ7vFOZYWIi7lAN2b66cpdR8HJURwhR9y7zZX2/UWdud1dAOrag/FtsEBgmQlRXGfkptFYjzIWC5iso5ViEFNLjSdQgEFEVEb1ERKuIaCURnZukPAwTx6oY+QjBu5y2NvdrzUQ9XmLjEj2F5DJKOnTFIwDeEUJ8jojKAFQmLA+T5+TR/JsUmXhhRghC2WWUP26eOHB73oV0zwVkD5IzCETUGcBFAL4KAEKIZgDNScnDMEA8q4yyCV2Ra70XdXFu9y6EyKvVNXGS262F2ZGky+h4ALUAniKihUT0dyLqkKA8BcGH6/bg8oemYsdB76Bnhc7mvYcx+sEpmLtpn/I1//fmCtwxfqF/whipb2rBl/8+G198YhYATfEdOdrqSCdTlE9MW48bHp8JANipR8hs9ok+at+HcPlDUzFj7Z6Q0gfjvleXYtCYCfjkn2egJWCUVCG08N6f/PMM1/NBuX3cQnzhsZkY/eAUrNtdJ00zfW0tRj84Bbv1+j3Q0IzLH5qK91bsCl6gIkFdRm1tAp/56wd4+oONMUnkTpIGoRTASAB/E0KMAHAYwBh7IiK6lYjmEdG82traXMuYd4z9YBPW7a7Hsm2HkhYlVuZs3If1tYcxfs5HluNeval/zNiI1xdtj1cwBWas25MOee2m0GUuj//31irM3qhuADN5ZVi3ux5js1AkQeZQnpu9BQCwdNtBNEiMniabPD8B4HBTK5ZuO+hTivyJy8R8Y/F2zNmktZupa+RG8cnpG7G+9jBW7dQMxpZ9DVi3ux5/domqGgkBjdvRtjYs2HIAv/jPCms2OfA9JWkQtgLYKoSYrf/9EjQDYUEI8YQQokYIUVNdXZ1TAfMT1Rg3xyaFdtdJPKZCaBvqveaIo6/ayk1/vCjG3cRBRwhJPr7EDIIQYieAj4joJP3QaAArPC5hioj8V2lquH0tK9pwE9bMsvHNhxUr6P0I4T0xnOtJ4zinM3xXiuXRJELSq4y+D+A5fYXRBgBfS1geJs+J+92JunfW6pJhnOoumzqK+v5dJ5WVa8DFZRROnESISlYh4jceiRoEIcQiADVJysAwceLmiojKraP1tK0k0uMMM0LwuCZXbpNcFGN3GeWzS493KjN5Ta6VW5SuCiGEu8soslJkZOMyithn73Y8z3RinM0sqMvIfVQVP2wQGCYmhJBHOjXOuV9XOJOQaRkCqqs2kZ3pKaSgf24T2fkIGwSmsIh5yBDpZC88Vph4lBNkwYss+6yqKE9cNcb5uEeIuXDfBC3BdanuMb7slAlFPvV9GC+8XUbuL7fbNR4lWf5KZArBY0ey2/G88qXHaHnst+lrDItx2SnDeJLQSxFlsQIZ5a7qJwaCGwR7XtnotlxVu98tqn4vIltyOamcrc3hOQSGsVFI4yPzSpoSmzbwerndlqpKy5Aci/ubEapyeB2HZHWUjFzdSayTyva//YxdfKL4wgaBYUxE6cYQEGnlnkrZDIJHOYFHCMFFc88rZ3MI0RZkX96bT5u9jGftJpKqAc/Fs2GDwOQ1efReB0aIjHIPMkLINoxCdi6jcGUHNaRtfvsQApYfZFRlKScHSjb99Tf9wfjPISQ3RmCDkCc0NLfg60/PDRTdMyiHGo/iq0/NwcIt+wNfO3HFLnz3ufkhJjyBVxduxY9eXOybTgiBH/57MX762lLc/fISAMCL87di0JgJeOS9tQCsyu774xZi/JwtGDRmAi7+/WRHfh+u34NBYybg8anrXcu8+6XFuOaR6Who1oKzGXf37vKd+MJjM7Fud730Ordommb2HW7Gx/84DQBgGyDg1xNWul/XoB4FfnddI24ZO8dyzM8gmNvaP2duwi//s1y5PC/21jfhlrFzcOXDU/HmEi3IoJtum75WLVClqnGrb2zxPL/7kFZPq/WgdmNeXoKvjJ2DTXsOByrHYP7mffjaU3NQ3+Rd7rJtB3HnC4u0MgDM37wfX396riXNT3RZ3NqaQS7CebBByBM27jmM91ftxm/eclcUZsI0jeXbDmHK6lo8rCvXIPzghUV4a+lO1DUeDXztXS8sxkvztyqlfXnBVvxr1hbH8YffWwPAqmD+s3g7xryyFACweW+D45pfvqGFxvrN26tcy/v3vK1YscMZOfbZWZsxZ9M+LP7ogPQ6mYx2Zm/cmzagXdq3s5x7deE21+tq65p88zaYsroWOw42Wo75uSDMbe1/X1+Opz7YlD6XTed05oa9mLqmFmt21eN7z3uHIV++3Vnnd4weijP7V1nkUJVn2wHvcPBTVtdi6ppa7NbrdvvBRkxbU4sX53/keZ0bv3t7NSavrsWGWm8l/pf3M1FU+3ZtjwfeWYXptvDku+uaMG1NLZ7Xo8fyHAKT5sjRYHHlg2D0ghpdQhV7Uaf3hOIczUadt1/vTQU3kVra/J+TOfJ1z84V6mXaCj1rUDcAwHkndE8fe+Czp+tpZRsR1MoxRkVR4P7oPJadms59+ZwBuOuKE3HXFSeqlRdRW1F4jFJk37iQ0dSSSVeSIs/r8uFTm8oGgYguIKKv6b9XE9Hg+MQqXnLiP8yiiDilU1p1EvtGJfvf4e/YPBcQRG5n9FLtZ4nZ76T/KvPgFcC+NGWFrlpv2b42YevMr9xW83mhpvTd93SoyxUWJYNARD8H8BMA9+iH2gH4V1xCFSOqKw2yUYhJT9Dm1UakHGCe6AxS9/ZaMp55yvTwjd/DzOnEQeDw1+n/NJzt329ppvW8XdE6c4unnvwms+1tXunDcgWwMe3TAK6D9lUzCCG2A+gUl1BM/pKNUlf9sHxUhDOewuMvU94KKr7VMkJQF8ap3LRrzSOEFMnTBi3LTtTPIGhvN3RPPfR12d2vX32Zn49QSJ80qgahWWh3IgCAv31c2GTzEmTTnH2X2ynkkfNRThY33BZ2hGArM6W/pRaPEcnTBi0rKoK2KeFzRXqppvIa/WyX6oarNb8ev32OQkXMXH8cyIyqQfg3ET0OoIqIvgXgPQBPxidW8RJnByKbnmMU5HvvCIi2/ltDziG4jRBUXUZZ7UOI+BG57mC2nTBkDjtn4B8KI9hxVfxcdmaXkhAi9H4J7frQlyqj9IEcIcQfiOgKAIcAnATgf4UQE2OVrMjIZa8gm4aV1bUx5i3DT7moGCi356KiuCwGIUC/3W0OwewyMoy7TMEkYvZDPLton3fIjWlZluo3SWxvY9lMKucCX4NARCUA/iuEuBwAG4GYidMwRDFAyMrd5NeLy4MPI9olyObltLz8WcwqG8rfHP7C+K0QRl0yoo7nk+3cethXw0/Bm+XS5hBCFoQ82ZgmhGgF0EBEXWKXpojJ5Rb6pIiiQUd5C3HXh9m/HEThuK2YkbmMpMtOs5pUDnldwPy0CUnnHIt9JOV2K0ENt7t82TUCP5eR/VkqjRCykig7VL+p3AhgKRFNhL7SCACEELdnK4A+ApkHYJsQ4hPZ5sf4k1WDi9HdpDThFqB8PzeNLKugseu9aDXNKAbbh2D92xgYlJDzmHQOIWD+SRBFW7CmtxlRxQqP22VkGSGIeEfYUaBqECbo/+LgDgArAXSOKf+CIt6dwPnVQy8Esqky6wghi2WnMpeRESgt4mVG4YPbBctPQET6tbewLqNM4Llw1/uvMjIvOxVKO6OTdAOqTio/Q0RlAIx95auFEMGD2tggon4ArgXwawA/yDa/QiZ4jyhEGelrk+mlRDJCCF+8pDxnbnYFlk2PzjzhmwoQJMYxqaz/LLG4jJxlZNInvQXRH+cqI7nMbnfi+E5xjrsqRnlBXUZqCxmCHY8S1Z3KlwBYC+BRAH8FsIaILoqg/D8CuBtAfAF8CpANtfW499WlOCyJxWO8IP+YsQH3v7kC97+5AvMUI6QabXHBlgOWhvnOsh340YuLUdd4FNsOHMEXHpuJQWMm4NmZm5TynbtpH/7vzRWWPP+7fCcenbzOku75OVvw6OR1uOeVJdh1KBOQbb1+vw3N3rGHLv3DFNdgc2Z+8cZytLS2Ycu+TMC7yat3Y8GW/fjlf5ZjwpIdeMwlAuqd4xdJz81Yuwc/eGERdtc1Sq6Ss3ZXJiKqm5LevPcwfvjvxbh9XCYYnJuylK0yMqJ3ujF1TS0eenc1GppbcO+rS7HeJxjb0dY2/PS1pVipB/w70tyKe19d6ojEaUQz9WPSyt1K6ewYbclNCb62cBsGjZmAJ6dtwJ3jF+I7/1rgSPP41PV4bOp63PPKUtegjIs82tPj0zZg/mbnuyWEwLJtWv1s2ONdn+Ygftv2H8H2g+7t5+kPNzmiHXvJFweqLqMHAVwphFgNAER0IoBxAEaFLZiIPgFgtxBivm5w3NLdCuBWABgwYEDY4vIecw/nkUlr8fqi7bji1F649KSe0vRzN+3H3E1aGOu3l+3EB2MuC1TGocaWdATOMa8sxYGGo/j8qH54b+UuzNEb5c9eX46bzx3kmofBneMXYduBI7jt0iHo1qEMAPA/z84HANx26ZB0uv97c0X69+pOFfiBHsjs4Ylr8OaSHbj4xGpP+TfuOex53uDpDzfhk2f0sRz7zVsr0dImsKH2cDq65zcucIbjWrWzDr99exUuHNoDQEY5//7d1Vj80QFccWovXD28t1IfXGXE89jUDXh5gT0SrPXCz43qi52HjuD0flUYP1eLztmzUzkAwzBY0/fsXJ7+/etPz0Vrm0DNoG54fvYWHDxyFN+5+ARXedbtPox/zdqCDbWH8fy3zsHCj/bj+dlbsK++GY/dnHnd7dFM3Xrov//vaulx4ZBaw16vg3t0cERzBYA/6VFEf+0RHdgc5faCIT1c02nlyp/oD/69GFN/fKnl2N7DmfDkLa3eD7msJIVm3a+k4ta6Y9xCvP69C9J//+SlJf4XRYjqQLadYQwAQAixBlo8o2w4H8B1RLQJwHgAlxGRIz6SEOIJIUSNEKKmutpbYRwLCAAHGoJ54/xC/1oyl/x+6MjR9CG/CJgyJWeUH8QVZY64WqfHsvd7uYJgH8o3tbRhxwH13r2BkUvTUev3EpRkMO9UtumbyrISAJCOiuyK46phvfHm9y/EaX20abahPTuivJ37q1temjln1IMhy6EjR70/TKOf3HfY+k2GA0fUv9GgguqqoG4dylBVGVzVOL5hHdLhImszcbr47SOInaaRdC7mFlQNwjwi+gcRXaL/exLA/GwKFkLcI4ToJ4QYBOBGAO8LIb6cTZ6FjPlZG37HVMQ7i632IPOXoYC0VRDR5O+HebLNKy5PWFpss32ymgw1DxPgmjaPWEZGPrJn7BrnxwhXYUkrmQeRLp/yklSS3CZf2J3AHlcoTYgL56FQRDmvYn534tDRSe7HUXUZfQfAbQBuh/Z8pkGbS2AiRghhehnVrgmz3V/WkFUaoleKIArdrGAySidCg+D4xi6FW0niMnmpsqzRK0xBOh+Pc3bMSs34PcxihCAKJ8gGuCCyBFo+HEHHyDcLt/MJz8+b5c6FmVA1CKUAHhFCPASk9w6Ue1+ijhBiCoApUeVXiJgfdtARgmqbtfRs/IRwy8PT36AoiE2WdBiGCEM525UxUbiQyPYUQRRZq2QUZM9HpuzcqiET0M5bCPkAQU3w9Eo0/TdjqauKiyfI0xPCO0+RESQSnRx2tC29ytyxikNNJzdAUHYZTQLQ3vR3e2gB7piIML/khkGIMxadfMlllstKA5Wf+T3jMgpfth37fES2LiMjbSC3mHkOwZ6f/lM2CnRT+DKXkeqoKiO/fP2/PV26zADlBPFx2xVpOridraYERCTvQZTvktVlF12+Msxi52J7gqpBqBBCpNdX6b9XxiNScSOQCZmrPEJQ3ZVp6dl4n1fJw04wl5G596y7jKIcIdh2AKWIQrkeHDuXA9yj2Sg5yg4xh5AyWQSZcfDNwPuUnp+wpDOS+88heBuabOSIwmXk9y65eowScBklOEBQNgiHiWik8QcR1QBQXNrCqBCm12eg7jIy/R5yDsEz/yATrpJNW9mEBrbjnEOQ9NKzdZH5Xmsq334uPRchuc5tDsHLCASQxet6pwE0fvqXGqTduqWU1Uc0LqNw18kmo1XrMgoox5MIqnMIdwJ4kYi2QxOrD4AbYpOqmBEmP27EDcD8UsuUjnY6CwUYSJbM70ajj3JS2T4fIXuxgxgge89ZSQaPZaeecwgu2zQzE8mm5yg17O7HVJd7ZtIL1zwt1/nMCfild1sFFF2TiGeVkZ+AYTpZuXANueE5QiCijxHRcUKIuQBOBvACgBYA7wDYmAP5igZzIwg6uaq8ysj1D9dDzjSSREb5QVw+0lVGEbqMHHMIBIdOUKlne4ogL7h1lGL3jWtI5xBc8kuZRghej9x73t9luWf6Wuu5jMsoeF15p1WzTAIi1ISw/Qq/LNzOJ+Myks895UP468cBGDtSzgVwL7TwFfsBPBGjXEWLgLlXptYAlNdY+84h+PuBo2qUZsVjRPG0u3myocXWzSZy1pKSQRDynypY9yHY89XOyZSdm/L1+mymH+k2FfBaI71KULZAIzwhb0uylpzkh/78FiMk6e+PAz+XUYkQwgiucQOAJ4QQLwN4mYgWxStasZFpWmk9otralO2BugJ0FEEe56BPhoecVM64jJQv98UxhwCneybISCyMaCrfVJb6tl0L011GPtJI3YGydLJRokvWSs82i+fn+t2DiNqEXz5unSqZS8+cVSwb0/LVZQSghIgMozEawPumc6rzD0xAvF6+qL6Xq+p79srDIBOKWV0Wsy5WXV8fBLv7SRZtNIwbJOP795fB65vKIn1cpnDkcqWyGiFkyvW63C3aq5o9iM7AmvtEUe/YL1TyYdnpOABTieh1aKuKpgMAEQ0BcDBm2XLChCU7MH1trfTcml11GDvDOlUyd9M+vDxfC0a2/3Az/vL+WtRLopIavLdiF95bsQt76pvwp0lr8dDENfhIj8K5ac9hPD51vcNVY0SaFAAenbwO3/rnPOw/3IyHJ67BPD2gnRkC8MqCrbh93EK8OO8jPDp5Hbbr8YXW7a7HneMX4rnZmzF5dSbyZKsQeGzqemzZm4kI6jaM37K3IS3f0x9uwl0vLMLmvVqguSmrd6cV37i5WzBxxS68vmhb+tq/TZFHFX1p/lY0Hm3FgYZmvLJAS3//BPdAZUHZZotBs2zbIRw8Yo0RdcQjbtOGWu3+Hnx3NW57fkE62ufEFbvwyT/PwAfr9vjKsGqne7RTIYAnp22QrzJyHaVpiVM+u66F0KKcvrowEzRvxXbtdZ2zMdN+ZSzackDPw2RBAKzeVYd/z/vI9bpr/jQd7y7fZTkmi9abkdHtewjajf1n8Xbc9OQsTFyxy5lIAXu7e2+ldz4zN+zFvE378N6KXZhkSlvf1IIvPD4TtXVNAIAV2w/hnx9uSp//y+R1lgilxvvqF7nXi5+/sTz9+97D0caQ8sOzly+E+DURTQLQG8C7ItOFSwH4ftzC5YLbntfC5m767bWOc7ePW4hVO+vwmZF9UVWpRfG84fGZaBPAZ0f1w7OzNuOhiWvQrUM5bjpbHon1m/+cBwD41oWD8eR0zbis3nkIj99cg3tfXYoP1+/FNcN7W9Qw6f4ZITLRIh/ouBrj5myRlkGkRWUEgDcWa2GJ1++ux0M3nIkfv7QYC7ccwGuLrOGKN+05jN++vcqi2Nx6eOPnZsp9Wn8Z9tQ34dlvnI2vPjU3fe7xqRvwODZYrv3dO6vgxsItB5RDdwdl3+Em3zQb97pHTzUC9jU0t2LCkh1FjgNdAAAgAElEQVTp4+8s3+mbb6eK0nTAPgOZAv/1WyvxxbOc7catI1jdqRx9q9rjriuG+spwy9g5lr+N6KAA8NxseTsCgGdmbnaV5e6XluALNf2l19U1tmDSKmuo67eW7pCmtecrw4jqCiBQyHEDrzDTbtz099lobnFOlszZuA9PTFuP+649Fd95bj42mztRAH70ohYRVQiRfl/fW7kb19ki7qriZgRz4UlS+abyLCHEq0II86cz1wghnAHIjzGMHp556G/2RBhRSVV6A3vrM5Z+635N2Rix6c35CyGk3kyvUYgMI0riul3yeO1GSF5zSGm3nulRyWeh7C9FGNqEcPTao0JleB2XI+L5b57jLMulsHYlzhNurqyO5aX4YMxl+PSIfp7lR6E40i4bn8yuHnac67kjR91HYEJY5TRqQVZP5/uErjYwwrmHRWYMDIyeuqzdG++2ua6aPO5dxvHVHQKlj4sA33FissLDNSBgbUyGQrBEJPWYAJVNiBl+VzdFJMvPbS15lJO99vJSYXcM+RBmSWlUBHF5y5S/mq84N351v3mBsO79IHWvWkRMTUnP2z9zyxfyAlaMSup8Cn9d1Lg9hrBLMNPr9tNLGU0jBMvxzDX2ZZSy/GTH3Lb9G6twVJbQRblhzIyAfDQUBSpLWON6wUoCaCaZmNnKFcltSdqg9nc0dabNIajlpapc45x8VsnZEsxQ16yq1RVFeI4oYIOggO+StZAP07wLVPZymBVx8M1qxgSk/LwsPyHkJi6OMBdGvnG9CF4G1Fx+HMgMgvtOXLUlokGIcgOTPSeVHeCZc+r5ZoLbSfKJOKZXGFSMjSw+lypKI4RAOYaDDYICbi9YEIUie3HSriGX5aBZ7V42fvqOEKyjExnxjRDi23Sk8vW1uAxCENeFzG6p7NjOVYfSbrAiizcl1BWcan2WxKjNvFd1aXeSTfj2PBkgsEGIgrDPMtN+XHrmpqNeLhCvcBJushnRQM2XBtmUFIVecJtAj4Ik5xCC9A6ldZutAFE8G5esHOJ63apHPfgF8DOj/l2QGF1GClnLwrEo568gez7sQ2CASF4wc/uwBymz+vHNK44yx7166bJzmUlleUOTLBzSSs/lpDLi8/vm3RyC2+R+Dus7FDZZovqIkX0Bg5e7R7WJxDmp7C2fvtPe8kEkY1e5av6hRYsUNggKxPV+uq0yMjC/e14vonSEYPx0nUNoc1zrph+Vv9sbFBHfi1AoIwR53Sq4jLzyVC7dIw/JSjdA8iW60Pmrp1WeVI7RIihNKlvmEKKXIR+C2zEKqLRXWRLzHII8HLXapLJshOA3qSzrQQvI5Yjy05bW8kRsE4GyvROO8uOaQwi0yii54UCY2FZBItJ6Gy2ttalcpT5CSHhS2RKqRB+hK+bPq4wKiCA95yD4foXK9Htwg6D/dGmSRuN1KIUcuzCSnEOIa4xQEmSEIDuWpVhxrle3V2tUiswrF/Vlp5GIIiX4HIL2U9llFFiieEjMIBBRfyKaTEQriWg5Ed2RlCx+uK4y0n+Gn1Q2KWVJEZYRgkfLkuk+f5eR9wonqxzuZWeDiNNlpBK4LrYRgvOY223GtaQ3KuzyBRkt+sVbUr1GVdEnvg8h7tFeDppFkhFLWwD8UAixgIg6AZhPRBOFECsSlCkU4fchWH96/W7/RrAfqbTLyHvZqUUeF1UU27JTEeOkssKy07hGPtmuMlKRy6vNRbICzPbTIEhb8Fo54zZvJs1HeR+CWn5hUJHB7DIK+gjyxGOUnEEQQuwAsEP/vY6IVgLoCyB2g/DRvgas2VWH0af0Sh+rb2pBx3KtOuoaj+KP7601yar1jP6zeLslj1cXalE6X1+0DTefMxCHm1vw4rytWFdbj5vOGoBhfbuk079oijBpPHwjntCqnYfS0RSNOEcA8KYpqNqybYcC3eP0tbV4fdG2dJA2O0ZkUXO8mZ+9tswRz+VQ41FMWW0NWhYVAvENlZdu8w/G++aS7b5pwiBzGbkpCJlSNLezMDw7a7NSujUuca4ALRLsqwu3otQ23Hlp/lb0qapI/+31/Lzqd8baPejXtb2SnPkwh/DivI8w5uqTpeeMOGPmd+nVBduwdf8R5RGViui5GDfmxTcNiGgQgBEAZkvO3QrgVgAYMEAeUTQo3/rnPKzaWYc191+dPvbUjI34/mgtiuTfpqzHP0xhrwW0iId3vpD5JtCFD0xO/75gywHM37If09fUpqNKPj97izSCqoy7XlgsPT7BI1qkH4ebW3HHeP9vGJmjcu5vcAaa+8Uby6XHAWBvvX9EUS+EEChvl9w01tvL/COXhqGyvMRxzAh5bkfW416+PWP8v3LuwMDlN3kEaVNld10T7nphMS4/pafluBHN02BQD/egbB+u3+t67sjRVjz1wabMAY99M6qKvrElWEC5IBxubsVMj/tpaxOWQJHvLN+pFBnXIM49FEFIfFKZiDoCeBnAnUIIRzdYCPGEEKJGCFFTXV0dSZlGFFPzy7j9YKYnvVny8vpFND3S3Grp3ecr5w/pbvm7e4cyz/RrXXqRQghlxVPmsoVUACiROdxtmA03AFSWORWuF1ee2ss/UYSUl5bgsS+PtBwz9xSrKjNROY02eOFQa0TPspIU1tx/NX553WnSMlTVx9XDjkP/bu3Rvp1anZWVWp+HXzz+oT07KkqiMXJAFa49vXega9zmEOz7PfyiwJrp1bkc1w63ylHqM1nR7LFyLds9NUojhBwMERI1CETUDpoxeE4I8UqSslheMfvCG5UJSgTwdQaQKmrKS62Kwc8n7LXBS3U47FaGtjnJPw+7kgr6YiTxxa0KDwVsdim1CU3h2Z9LWWkKZaWprFfxdK5oh/LSEmXff5BlpWZUjXRZacpT8cru1+352bPxU+iWckAOmf2q2qutCiGymmvLj/FBsquMCMA/AKwUQjyUhAyuq2rsnxFUfM5xxlKJCnvD83v/3RSEgPoEo8drlJteTwLPxSviqXmfghDaXgy7MopqCWUqpTkjVOvZ/kxVxVCN8EogqYL3DJLncsp+T0GizIbBqw6DTJJLUTD8x/rGtPMB3AzgMiJapP+7JpcCmCvY/DzCPth88QN6YW93fkrdaymdamfSc4QQopEHvSaJ5+K1F8E+QpCl9FNuqgOHkpSWNnvj7Z0+WO88GKqjpKAGwX6vfu3Eq72rjnbdyBfNkeQqoxlIuB6i7J0KIdR7oomuMbOW7VcHriMEoe4ych+J5cYvmkQr81JiZr3VJgSInCJG5eZKEYFAyoo+6PMwlKDyCIHg+Tzkk8ouZdv+DmSUQlSvV0fEbcl2lPIc83MISaO6FFD1QSThqw5KtCOE7FqoEOH2AhTCHIKXgiwpsY8QJC6jiNwfKd0dFfZZRd07J5I/D69i3J6fvUeeqMtIZKew80VzFLdBMD1B8wNxzCEo2v6CMAi2v/16+V4bvLKNcSSy7lepkcRT8ZpPMruMhNB8RnZ3hV/4C1U3mKEk49xtDsCxX8GLoM/D1SDY/g4yQgAkdeJzuZ9RzWpSWWkOIX6K2iC4IQ0noPA0Yu6gRIK93fm6jGIcIaiUHwVJ2Gm7EjP/aZ1U1vSQXcaoerslKcpqpZLflYZBV7UHstGQmSBtyp40zminKmTlMopMiuwoaoNgfoCWSWV7uoi32OcTfvFXvJadBoym4SDsRFzQK5IYuXmVaRkhBPhQTBiyzcd/Kab2U3WE4OYy8kI1fZCggoYslr990vu7jHLRh4+X4jYIpudnHoI75hCg9mIVhsvIKqOf28d92anIOphXriaVk3EZeRgE07m2Nq1txTZCoHjXWBnPL4i8stfEOCRrDuqf0MzOZeT3+nqNXkSWS6jVJpWP7WWnyeNav+EqvhBdRn54Kf2s5xCECBdgLuA1SYzcHC4jk1o2n2sTQnOjBJ1DCKAks7l9P3MSatlpwMB8qq6g0pJ4n7PvCCELp1G+LFkvaoPgtg/BkU5xp7Jqw0100WlQg+AyqaytEIpgDiEXk8oJVLjqCMEtBHjUq4ziIviyU1UTY75GTRaVMCjpPKXH/PYheI0QsnSh5smy07wIbpcLjra2YcHm/agZ1C19bM7GfZbfN9TWozSVclT8C3M/QjefmD+As+FOWrnLNe2SrQfUBI+YoD2RuiZ5DKfddU3YvFcesE2VORv3Yca6PYGvC74xLfd4rTIyK/s5m/ZpytS+7DQioUtSztFHEOZs2uefCEF2Kru4jPRj8m97xDOHYG9H5milMtbtdo8OO37OFkxaGT4qsIrke+qb0L9bZegyVCiaEcJzszbjhidmYeKKjJK+9dn56d9X7azDZQ9OxbV/nu5QN49P24DfvL3Kt4wOZVb7+o1n5knTNbe04bq/fKAufIScfFynyPL60YvyKK2qPDd7i69RMYK+GaHJrzy1Fz5f01+5jP7d2mPUwK7hhQxJ54p2lr/NRqBLe+u51jaBAw3WIHL7DssjzMrw0sX1TS2xjRDOH9I9/a7Y78mNPfVNnnNtsmCL9U3yuvjsSGswu3Y2l5G9rZuLDRpgD9D0gBv3T1iJmRvco6H6cdGJ/oE7tx9oDJ2/KkVjENbVata9ts67UusaW8JN3gigQ7nagEvle792XrvtfMexh284w/e6Hh3LLX9/6ZyBjjTXndEHH465LB258voz+1hezKk/viT9e58uFfbL8fw3z8YNJiXdroQw597RvrK5MeVHl2D63Zdi2o8vxZNfqQEATL/7Uky/+1L8+aYR+NV1p2H2vaPx3g8udlw7857L0tFN77n6ZEy4/ULc8LH+mPGTS33LPWuwNno06qFvVSZe/68/PQwjBlRZ0r/y3fPwwGdPtxz7y00jAAA9O1dg1j2jMe+nl2POvaMtSrtHxzL87rPDLdeN6G81Wqf36wJV5t53uSNaqvHcR/Svim2E9ODnz0x7eI7r7GwXBubIryMGeMtzfHVH/PfOiyzHagZ2w9z7LseCn12RPnbh0B74ra0Oh9gir77xvQvw+M2j0n/Pve9yTPvxpXjnzgsx5upTPKSw8p/vXaCcNgiP3zwKH465DB+MuQzfveQEy7kTqjNhxd+6/UJMuP0CXHRiD3sWkVM0LiNjKBrnBhBVQxJmdY6sZz+4h3/o4b5d22OP6bsFsrs/rksF+lS1x8m9O2Pt7nr071ppmaAb2D3TOKsqy7D9oNWontK7M+Zv3p/+u3+3SvT0UBB+yGLsd+1Qhq4mI9WrcwV6dXZe27tLe3TXlWGH8tJ0T71fV/+h9ml9OmPOxn3o3N64pn36A0Mn9uqEnp2sxrW6YznqG60utU6mkcFxEuMJaC6Q47pYPw5jj+jauUL91ezesdzRs+7eoQx76ptQagQzioHy0lTa7eJVRLkp8muHslLfsOkDbG6R0hJCtb3uO5Wjnc0vZ1/6Wlaawqm9M42kR8dyIFi07nRZcdC3qj36VMk/EnR8dUesr9W+r3BqH0lDj4miGSEYyjofYobkcrmy/Xbl36wlS9qg+iOVIqvvN+Hl2Ib8YcWQ+aLdqkQ1FIhjiaPtvN3t4zep7CxXnq4kFd8cClHmvrz8/GYXkSyyK2BbAu7YH6B2B4Gj2irvLwqYbwFTRAZB+xnXXoEgE51RfaNY5U7sekUeb944l8nXTUS5QcnNaiFV0vccOn5P+GNuOKIk+DyXoO3UsZky3XOPb5WRatA8c/GagQq6MU1etp0g4TOCEKdBzXWZfhSNQTCUsMpiiPAuI7V0Ydbvh1VIdkUju8ZYIZJWQgE1SIl9hJAwhrIIKpNxnVwZywypZAWPaq8T3gYg6Cojt05GnBvTKGUaIXgUYr63EiLpvQmX9FrekhGbRzv2SxeUuPax5MveAzNFZBC0n0pzCCF7lqq95DBfpQrbeByuCOkIweYy8pLDxeWUT9v2jXsOIpP5vlQVi+yY6+jPHEiRnNeqPCdr2dbz9ns17yCOT6FBaQ7BfC6My0jVOMo2x3luglPLNr4RVv7Zg2IyCMYIIfmnEKY3Ld/u738v9jSyK+wKMGgVaSOE/DEIhhIIPELwGCDJxwzO+w5bDV7B8FSwb4oyxEjFOUIgMt2veylWl5G/gXLOr6jN6cQV3C4Jl1FSFI1ByMwhqKcNW4Yf2cYAMlBqUA5ftTNJuk5CNtAU5ZfLyCCISATzpLriJCbJfPdqOD5V6Sgy2MNwM8hxhlMx379nldlGX37V63CfKWop6chOTaxA8hzLFI1BiHuEoMUyUZQllMsoHEouo5ThMsr8DCJhiqKbKI8C4x7DurFkIXGkRoKcz9KtTPNRgnP0kv0cgq08U0iJOCeVjV6QVxHme5POu8A7jIyqgZa7jNzTJ+4y4jmE5MjMIfinDfWdX6E+QgijPMP6gVUanX2ZpffqB6fisroOEl91mll2GkAQYbpOOokpuUYbGdkMglv+thP264K67eyn3QwRuSjgKCDFEYLVILgYO8scgr2NhZ9UjoLY5mDyzx4kaxCI6CoiWk1E64hoTJxlGS+g2qRy8PyDXBKVe0VtlZH/NcaLpLLs1O3aMKOeuDBuMYhh97tft3kFZ89ctTxvl1FQJe7WySCzLyxiVPch2CfsgypCt/kbO/L9I9nffHwjhNyX6UdiBoGISgA8CuBqAKcC+CIRnRpbgTHPIQih/jnIXLqMnCs2ZL2tACMEl+F8/piDjAss/FyQ80KZYiHJCMGtJhyfZbVvVAvpMkqPhlykiHVS2ZSz9wjB/Luby8jrerU7CLo0W9WlyJPKueEsAOuEEBuEEM0AxgO4Po6Cth84gl2HtHALa3bW+aY3h3pQZfPeBs9oiGbcIoh6EesqI6mSV3tZjF6ZWTEmvQTVuJ3gq4zUerkGKXJf7mnH6TKy/u1023k/W/tpt3vVXHqeWYXGfP8+0lquke5D8Ggz0o1pqvM8ERDfpHL+WYQkDUJfAB+Z/t6qH4ucv01Zj3l6rJ0/vb/ON/1aRcVu5tdvrcQrC7YFvi4bwrRTWeM2Yssc1g1Vk08YYDNGQL/jJfGHwtBRMUCgnXJTLKDBuix9qqyxhOzRMM0YAfEApGMZnTmgKp1X+7ISnNZHCzjXSY8zVFaaQtdKawyhqkr/MOkA0LUyE/Po7MHdcFwXa7wc1eihBqf37WK57tzjuwPQ6tMehTcqUkTpeE0n9OzoiMdkYL6X5tY2HG52tq9OFe7321lSF/ZAdm5UlGpxlGSyndzbP0ZQiqL58E6nilJHoEn7q2gOppjUhHOSwe18ppb0RES3ArgVAAYMGBCqoJvOHoDeVRV44J3Voa4HtOiK553QA797xz8MdlA+cXpvvLlkh+XY379Sgx+/tBj7G7TQv269n3YlhKOtAt+7dAjOPr4b+netxM9eX4bpa/fo11nTy7I5UQ+cZwSVa2ptS/don//m2dbr9Z+fH9UPF59UjRN7adfeeNYAbDvQiMemrk+nnfTDizH6waneNw/gze9fgNY2gbW763HBkHARHZ/+2lno2kFTHJ8b1Q8n9OyIkQOsEUQn3nUxNu45jOlr92DsBxsBAA994Qw0tbThmuG98dfJWmfh5OM64+ZvD8Swvl1wy7mDsGVfA47v0QH/c/HxOO+E7hjasxO27GtAZVkpzhrcDeNvPQdHjraifbsSnD24G2TYN16NGtgVL337XJSkCAO7d0BV+3YY961z0Kgb47Nc8rFjPI/vXTYEl5xUjSE9O2Lr/iM4obojPjOyH46v7ohff3o4Jv1mEgBg5IAqLNiifYvj5nMG4tlZm9N5Tb/7Ulz4wGQAwE+vPQUjB3bFgYZmPDtzMyavrsUFQ3rg8lN6YmD3DqjuVI5UinDFqcfhxW+fizP7V+GiodWoa2zBgSPNqCwrRe8uFdhb32xR3icd1xlV7RvTstx26RCkUuSot3fuvBAdy0ux82AjTjMFd3vvBxdj+faDuHpYb/2ZXoQrHp6WPj/uW+fgi0/OSv/dpbIdXv3ueVJD/fXzB6N3lwrcMX5R+m+jXdz/qWE4s38VKstKUF5agpe/cx4+2teAinYpNDS3oneX9ujZuRyPTl6HVxZsw9mDu+Ga4b0xYkAV2oTmiVi+/SCembkZJ1R3wNivfgx1jS34xJ9nOJ6dwYvfPhevLtyG3/83vJ7KliQNwlYA5sD2/QBstycSQjwB4AkAqKmpCeWLOKV3Z7QrSSkbhD5dKhwRPS85qSc+PaJvYIPwuVH98NL8rY7jvTqXY9ehJpzSuzO+fsFgh0EYNbArBnTvgP0Nzg/plKQIrW0CRFrUz637j+CSk6rTH/85vV+XtEGwIzMsRu86HcPfVMtuUUs/PaIvzjMp73YlKXzyjN54bOr69OX9PSKMmut4mN67PaN/lWt6P6o7lacVT2lJCh8b5FSog3p0wKAeHdC7qgJjP9iIyrISfMYWUx/QFLZRl31MESnLS0vSx4dXdtHTEs7Re+NeOGMZkeVjTQBw7gn++aSvt6mTinYZ2QzlZ3wHwhx19eTendMGwR5i2/zxlZpB3XCm/jwWfXQQk1fXYtTArvjq+YMt15SkKF3Xsii1Rt2d1KsTVu+qQ0VpKj2q7NahDKNP6eW4BtCMMuCMUjukZ0eLgRnaq5PtOmdU4BED5N/DKCtN4foz+6YNwmUn98TYDzbi9H5d8GVbmPhRA7tKv6vRT7+/807ogVvOG5Q+fmb/Kjw2VfvGxehTemFg9w7YadMp9nexT1X79Ig0KZJ0Gc0FMJSIBhNRGYAbAbwRV2FB3CtuvfEwq9rcrjF8xkII6eoIr8nAdDA6UCbshG3iLgjp/QcqvlovP7t9vsJzcjqaIXE2Sw3drsyjLRWRY743L1cISX7PplqMOaaSFKW/JhdHPWfTrIwNcEGWhXulzKzA0n86VpK5k9SEc2IjBCFECxF9D8B/AZQAGCuEWB5XeVHUbxi/npvCMq+EkabxmAwkZCZ9M5eaV3wENAi25AKZhu6ak4LxyMUOzxQBrZKyvXB9jjGKG9dEezaG1ev5SDsYWdyDsTs/lcp0YvLN7qYXSIT4NrKsKs0RZwFn88rHVUaJfiBHCPEWgLdyUVYUPVIKMZ5yKze9fl8Il4BzXpkaecNlhBBQRttPyzn76hevfBLoAaUCrIhyyiAXIu5Q3smujc/cm2q4aKOsbLaapIPtEVnaftRkU7dh5PJK6hcJlncqJ0iQqnd/gMHxcxm1CeESXdO9uZgVuHkzWabMYKMFr8iZ7jJIZA7gMopq9GCP1KqC3wtaSC6joLVovrcSj7ffssfAuDYLQ2l2GWVCi4TOLhbC7F9JjwJk5+xLcu0dpvyzB8VjEILgqjBCPEE3xWcehbt4jJTK8xtdhFGUUb2oXvJHFWUgymgF+fiCxolyEL8sN/oBmW+AkMkVmmf2IP0uhQst4zwW90e54qBoDEIUzyTcpLKCy0iSsYp/1+oyyn4Owdwr9PvkaNBdoc60EY0QQjyUpF/PKPVD0Lwsk8oedSfLNxKXkSn8dSzzKlnUrWyTpR9eSe3x0/LRRWSneAxCFH7bCB9opjfiEoOF3Nt2xliQqbHJzqspDHsSa2ROlxGOxzGV9ymqmpQZRN+yI3QJqhK17gsrq9nto2pMM5PAWUwq69rR/MW0/HMZaT/Dfa/EWZf2SWXnNc5jSddJ8RiEIL1XNyUY4i10/bShaYTgthJJRXGlVzCY3UQB5TSPOAzcolh6LyUNXma2ZOMyymV/LZ++OW0g64jIiMKVmA4/n8rEMoplUjmbEULEk93pEYL+t/NdCjZCywVFYxCiIMqHZJ5Yk+Xr7TLKGAHznoTMtUGlCeF2kcoXpJcekcsoxKSyX+qkYzEFIeio1etTle5lGNdmM6ms/UwRRWJg4iDMZLdnUj+3q3oxOaNoDEIU+idSl1F6os5tlZH/SIUgH85bXEYKsjj2IXgoDa/8MpOF/m9UZC6jCIcIcfbKIld+IWU1ixF0H0IUI4QSs0HIs1GTcZ+tEYVyN0ecBZyPzGvvQlIUkUHI3sccRve4vUTGJlG3OYSUxySCdb4gk0/6fNBJZcmxzBpqdXdWkFKjdhlF6a6Ko+eaL6pPdYQgC22djZ40lGwqBZPLKHx+bmTTrEpMnTRVvCeVrctOnXt6PAxyQuOH4jEIAdLK4ol0rWwXavlYZVmJ9Hi5HoVxeL8u0kiMBKB7B3nkTCPmT7uSVDrmizmSp/k6WVAvc1RFQK70Rw4wysicKytJeRob4z6G9+3imsbAiByaLUa8nVKvRfUu2O+klx63qbstKmUUmAO02es/DO30GdCPDZbH6XGjX9dM2bIoqEYUVkt76qi1IXtE1iAY7am8tCT9TgxVjFjqR4+OmTZubLY7M0RcLCOm1/B+6m2zV+dyhwwGPTtp7alHJy2NPVSILPquEe3XHFcqlyS6UzmXeOnyf379LKzccQgCmpIdMaAKU1bXAgB+85nhaBMC157eG6kUoaqyHQ7oEUj9eOprH8OwPl0wqEcH3PPKUgDAF88agHFztqB3lwr89UsjUTOoK6oqy/D4zaPQq3MF5mzci6G9OiGVItx37SkY0K3SEVTrV9cPw4rth9Cva3uMufpknD+0B041hfK9evhxaGgejsajrbhq2HG4+MRq3PnCovT5Z79xFrbuP4KvjJ2j1Y1NbgGB3332dMzfvD/dqF/89rnoUFaKn72+zPV++1a1x6M3jVSK1PnTa09B54pSfL6mv29aL+7/1HBcd0bfQErWrSl8oaY/urRvh4+fdlxWMsn430+ciktOqkaKCJe7BHQLQtcOZXjsy6OUFd9rt50PQAsOV1XZDlWV7XB6vy74xy01aDzalg4V/q9vno01u+osAeSuGd4bBMKVp4WX22hP1Z3K0b1DGR658UxcfGJ16PzMjL/1HOw6pH3DpH1ZCf7+lRqc0sc/tLWdPlXt8dcvjZQGRnTjxo8NQLcOZdI2Y7Snq4Zp5zpXtMMTN49Cz84V2HnwiLTjccGQHnjkxjNx2ck9cc3w49CS4/ymBUcAAA0kSURBVC8RFo9BsKmBUQO7Yv7m/SgrTeGiE6txkalxGh+6qSwrwRfPsobcvuTEary2yBGU1cHwvl1w6Uk9AWhGwDAINQO7YtycLQC0F83AaFDmF7xf10rcc80pjrx7dCzDtadr13btUIbrzuhjOV9eWoKbzs7Ifd4QaxTN46s74vjqjkiRNmzPrDLK1FHPzhW42iSf8ZJ4zyFQWi4/uncsxy+vH6aU1ovqTuXKZdqxj3Yq2pXg+jNj+SQHuncsx6dHOCOrZoOhaFQwt6uvmSKW2qONntani2P0Vl5agk+NyK5ezO0plaJI63lIz04Y0jMT5fTyU8MbLvM7qUL7Mvc2077MWW9XGobDxZCXlqTS+Z3eL3z037AUj8soH6f0QxJ0jsB9t7Qx2WU9n2+rP6Imri9rMUyhUzwGwfa316f/8l1fBBXPL71sc9uxTLHcJ8MEpWgMgh23jVfHIq4rhRzp4peFYZj8pXgMQpBlifFJEQlBFbfbclm3fFSW3RWyW4kNH8PIKRqD4OYnL4SAU3aCyuyXPojL6FhSpsfSvTBMFBSPQbDvxvVMm9+aIrB4rhvc5JPKKhRSeAc7hdgJYJhcUDwGIWkBEsRvh3UgY3kM1eSxcycMEw3FYxAiCsGQD/3i4NFMFV1GATLOh3oIS54PABkmMRIxCET0eyJaRURLiOhVIop9B0YScXbiImgv3XWEkJ47CLAPIc/rJgj57hpkmFyT1AhhIoBhQojTAawBcE/cBTrefY99CPlO4BGC6qRyIVYGwzCRkYhBEEK8K4Ro0f+cBSDaPf0SgvSq891PHnhO2Sdqapi7LeA5ZTZ8DONCPswhfB3A23EXkrLdqRHZcmivTpLU2WOOKglkIhtWtNMiPXZziWTqRbUeNTFs6IreXSqk543sKvXol1V6xEsZRhA5e+TGQsKojyHV0UTbZJhjhdiC2xHRewBk0bfuE0K8rqe5D0ALgOc88rkVwK0AMGDAALdkvnSqaIdfXncaRg7oiunravG5kf1w9vHdpREXVfRtaYrwtfMH4dKTe2L+pv2oaFeCEQOqsHHPYew93IxP2AKujb/1HGza04DRp/TEjz9+Em74WPAon2Nv+RiWbT/o+slNN8pKU/j1p4fhDNdgWVp+Vw87DrsONToC+pm5+6qTMLRXR+Uom3/90kh0qihFSYqwt74ZbUKkjXFS9O5SgZ9eewouO7lnonIw+cHfvjQSHcqLJs6nJ5TUenIiugXAtwGMFkI0qFxTU1Mj5s2bF69gAD7a14ALH5iMyrISrPjVVZZzd4xfiNcXbcfDN5wRefTKXHPyz95G49E2TPrhxTiBe8sMc8xCRPOFEDV+6RIxi0R0FYCfALhY1RjkG/k+zxCEY+dOGIbJhqTmEP4CoBOAiUS0iIgeS0iOouZYMmoMw2RPIiMEIcSQJMpVxWsOoZBX19gJsyGNYZhjl3xYZZR3FJuCLK67ZRjGDTYIRUx6HwJbBIZhwAZBSrHpR55LYBgGYIPAgEcIDMNosEGQUCwKstjmShiG8YYNAlM0BpBhGG/YIEgoNp86jxQYhgHYIBQ12UQ7ZRjm2IMNggSjwyyLDtqxQtvLFzTAXD7Su0q7Px4gMAwDJLRTOd+p7liOr58/GJ88o7fj3HcuPgEE4OKTnFFSC40HPncGXl+0Db06JRt9lGGY/CCxaKdhyFW0U4ZhmGMJ1Win7DJiGIZhALBBYBiGYXTYIDAMwzAA2CAwDMMwOmwQGIZhGABsEBiGYRgdNggMwzAMADYIDMMwjE5BbUwjoloAm0Ne3gPAngjFiQqWKxgsVzDyVS4gf2U7FuUaKITwDa9QUAYhG4honspOvVzDcgWD5QpGvsoF5K9sxSwXu4wYhmEYAGwQGIZhGJ1iMghPJC2ACyxXMFiuYOSrXED+yla0chXNHALDMAzjTTGNEBiGYRgPisIgENFVRLSaiNYR0ZgcltufiCYT0UoiWk5Ed+jHf0FE24hokf7vGtM19+hyriaij8cs3yYiWqrLME8/1o2IJhLRWv1nV/04EdGfdNmWENHImGQ6yVQvi4joEBHdmUSdEdFYItpNRMtMxwLXDxHdoqdfS0S3xCTX74lolV72q0RUpR8fRERHTPX2mOmaUfrzX6fLntW381zkCvzcon5fXeR6wSTTJiJapB/PZX256Yfk2pgQ4pj+B6AEwHoAxwMoA7AYwKk5Krs3gJH6750ArAFwKoBfAPiRJP2punzlAAbrcpfEKN8mAD1sxx4AMEb/fQyA3+m/XwPgbWifYD4HwOwcPbudAAYmUWcALgIwEsCysPUDoBuADfrPrvrvXWOQ60oApfrvvzPJNciczpbPHADn6jK/DeDqGOQK9NzieF9lctnOPwjgfxOoLzf9kFgbK4YRwlkA1gkhNgghmgGMB3B9LgoWQuwQQizQf68DsBJAX49LrgcwXgjRJITYCGAdNPlzyfUAntF/fwbAp0zH/yk0ZgGoIiLnN0ajZTSA9UIIr82IsdWZEGIagH2S8oLUz8cBTBRC7BNC7AcwEcBVUcslhHhXCNGi/zkLQD+vPHTZOgshZgpNq/zTdC+RyeWB23OL/H31kkvv5X8BwDivPGKqLzf9kFgbKwaD0BfAR6a/t8JbKccCEQ0CMALAbP3Q9/Rh31hjSIjcyyoAvEtE84noVv1YLyHEDkBrsAB6JiQbANwI64uaD3UWtH6SqLevQ+tJGgwmooVENJWILtSP9dVlyYVcQZ5bruvrQgC7hBBrTcdyXl82/ZBYGysGgyDz8+V0aRURdQTwMoA7hRCHAPwNwAkAzgSwA9qQFci9rOcLIUYCuBrAbUR0kUfanMpGRGUArgPwon4oX+rMDTc5cl1v9wFoAfCcfmgHgAFCiBEAfgDgeSLqnEO5gj63XD/PL8La6ch5fUn0g2tSFxkik60YDMJWAP1Nf/cDsD1XhRNRO2gP+zkhxCsAIITYJYRoFUK0AXgSGRdHTmUVQmzXf+4G8Kouxy7DFaT/3J2EbNCM1AIhxC5dxryoMwSvn5zJp08mfgLAl3S3BnSXzF799/nQ/PMn6nKZ3UqxyBXiueWyvkoBfAbACyZ5c1pfMv2ABNtYMRiEuQCGEtFgvdd5I4A3clGw7p/8B4CVQoiHTMfNvvdPAzBWP7wB4EYiKieiwQCGQpvIikO2DkTUyfgd2qTkMl0GY5XCLQBeN8n2FX2lwzkADhrD2piw9Nzyoc5M5QWpn/8CuJKIuurukiv1Y5FCRFcB+AmA64QQDabj1URUov9+PLT62aDLVkdE5+jt9Cume4lSrqDPLZfv6+UAVgkh0q6gXNaXm35Akm0sm1nyQvkHbXZ+DTRrf18Oy70A2tBtCYBF+r9rADwLYKl+/A0AvU3X3KfLuRpZrmLwke14aCs4FgNYbtQLgO4AJgFYq//sph8nAI/qsi0FUBOjbJUA9gLoYjqW8zqDZpB2ADgKrRf2jTD1A82nv07/97WY5FoHzY9stLPH9LSf1Z/vYgALAHzSlE8NNAW9HsBfoG9UjViuwM8t6vdVJpd+/GkA37alzWV9uemHxNoY71RmGIZhABSHy4hhGIZRgA0CwzAMA4ANAsMwDKPDBoFhGIYBwAaBYRiG0WGDwBQFRNRK1iiqnlE0iejbRPSVCMrdREQ9Qlz3cdIihXYloreylYNhVChNWgCGyRFHhBBnqiYWQjzmnypWLgQwGVqkzg8SloUpEtggMEUNEW2CFrrgUv3QTUKIdUT0CwD1Qog/ENHtAL4NLUbQCiHEjUTUDcBYaBv8GgDcKoRYQkTdoW2Eqoa285ZMZX0ZwO3QwjrPBvBdIUSrTZ4bANyj53s9gF4ADhHR2UKI6+KoA4YxYJcRUyy0t7mMbjCdOySEOAva7tM/Sq4dA2CEEOJ0aIYBAH4JYKF+7F5o4ZAB4OcAZggtONobAAYAABGdAuAGaAEFzwTQCuBL9oKEEC8gE7t/OLSdsSPYGDC5gEcITLHg5TIaZ/r5sOT8EgDPEdFrAF7Tj10ALcwBhBDvE1F3IuoCzcXzGf34BCLar6cfDWAUgLlaCBu0RyZomZ2h0MITAECl0GLlM0zssEFgGGuoYFksl2uhKfrrAPyMiE6Dd8hhWR4E4BkhxD1egpD2KdMeAEqJaAWA3qR93vH7Qojp3rfBMNnBLiOG0Vw5xs+Z5hNElALQXwgxGcDdAKoAdAQwDbrLh4guAbBHaLHszcevhvZJQ0ALUvY5Iuqpn+tGRAPtggghagBMgDZ/8AC04G5nsjFgcgGPEJhiob3e0zZ4RwhhLD0tJ6LZ0DpIX7RdVwLgX7o7iAA8LIQ4oE86P0VES6BNKhvhin8JYBwRLQAwFcAWABBCrCCin0L7Ql0KWuTN2wDIPg86Etrk83cBPCQ5zzCxwNFOmaJGX2VUI4TYk7QsDJM07DJiGIZhAPAIgWEYhtHhEQLDMAwDgA0CwzAMo8MGgWEYhgHABoFhGIbRYYPAMAzDAGCDwDAMw+j8f9m+LmaFYsjkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b880e6cc0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "scores = train(agent, n_episodes, max_t, eps_start, eps_min, eps_decay)\n",
    "# save model\n",
    "torch.save(agent.model.state_dict(), 'checkpoint/prioritize_dqn.pth')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.savefig('results/prioritize_dqn_result.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Model in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "agent.model.load_state_dict(torch.load('checkpoint/prioritize_dqn.pth'))\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = agent.act(state)                      # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
